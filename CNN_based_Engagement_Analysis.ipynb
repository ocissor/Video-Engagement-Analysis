{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycm import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import struct\n",
    "import torch\n",
    "import shutil\n",
    "from torchvision import transforms,datasets\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import PIL\n",
    "from skimage.feature import hog\n",
    "from PIL import Image,ImageDraw\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pycm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot=OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(src_path,dest_path):\n",
    "    files=os.listdir(src_path)\n",
    "    while True:\n",
    "        if len(os.listdir(dest_path))>=412:\n",
    "            break\n",
    "        i=random.randint(0,len(os.listdir(src_path))-1)\n",
    "        img_file=files[i]\n",
    "        img_path=os.path.join(src_path,img_file)\n",
    "        img=cv2.imread(img_path)\n",
    "        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        img=Image.fromarray(img)\n",
    "        frame_draw=img.copy()\n",
    "        frame=img.resize((100,100), Image.BILINEAR)\n",
    "        img_dest_path=os.path.join(dest_path,img_file)\n",
    "        frame.save(img_dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_embedding(labels, num_classes):\n",
    "    \"\"\"Embedding labels to one-hot form.\n",
    "\n",
    "    Args:\n",
    "      labels: (LongTensor) class labels, sized [N,].\n",
    "      num_classes: (int) number of classes.\n",
    "\n",
    "    Returns:\n",
    "      (tensor) encoded labels, sized [N, #classes].\n",
    "    \"\"\"\n",
    "    y = torch.eye(num_classes) \n",
    "    return y[labels] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=torch.LongTensor([0])\n",
    "labels=one_hot_embedding(labels, 3)\n",
    "labels=labels.reshape(1,3)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:13<00:00, 14.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3123, 0.3314, 0.3563]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.2984, 0.3452, 0.3563]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 0\n",
      "tensor([[0.2634, 0.4361, 0.3005]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 2\n",
      "tensor([[0.2833, 0.4134, 0.3033]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3251, 0.3498, 0.3251]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2511, 0.4979, 0.2511]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.2996, 0.3456, 0.3549]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3080, 0.3841, 0.3080]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3281, 0.3281, 0.3438]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3102, 0.3292, 0.3605]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2747, 0.3369, 0.3884]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 0\n",
      "tensor([[0.3064, 0.3358, 0.3578]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3099, 0.3099, 0.3801]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.2782, 0.4312, 0.2906]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3157, 0.3379, 0.3465]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2883, 0.3607, 0.3510]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.2817, 0.4094, 0.3089]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3132, 0.3294, 0.3574]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.3038, 0.3289, 0.3672]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.3328, 0.3345, 0.3328]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3028, 0.3272, 0.3699]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.2892, 0.4087, 0.3021]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3159, 0.3479, 0.3362]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.2927, 0.3511, 0.3562]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 2\n",
      "tensor([[0.2999, 0.3890, 0.3111]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3310, 0.3380, 0.3310]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.2769, 0.3845, 0.3387]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3066, 0.3440, 0.3494]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3171, 0.3429, 0.3400]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3134, 0.3322, 0.3544]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.2789, 0.3753, 0.3457]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2825, 0.3827, 0.3349]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3255, 0.3490, 0.3255]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.2972, 0.3381, 0.3647]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.3216, 0.3216, 0.3569]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.3036, 0.3395, 0.3568]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3163, 0.3242, 0.3595]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3097, 0.3373, 0.3530]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 0\n",
      "tensor([[0.2922, 0.3788, 0.3290]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.2963, 0.3939, 0.3098]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3057, 0.3307, 0.3636]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.2818, 0.3243, 0.3939]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.2759, 0.3770, 0.3470]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3136, 0.3556, 0.3307]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.2958, 0.3652, 0.3390]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3317, 0.3366, 0.3317]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3323, 0.3323, 0.3354]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 0\n",
      "tensor([[0.3003, 0.3710, 0.3288]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3050, 0.3769, 0.3181]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 2\n",
      "tensor([[0.3008, 0.3392, 0.3599]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3065, 0.3556, 0.3378]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3327, 0.3335, 0.3337]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2780, 0.3432, 0.3788]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3162, 0.3452, 0.3387]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2670, 0.4237, 0.3094]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.2759, 0.3760, 0.3481]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.3147, 0.3147, 0.3706]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2903, 0.4170, 0.2926]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3014, 0.3932, 0.3054]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2892, 0.3745, 0.3363]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.2944, 0.3494, 0.3563]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.2789, 0.3281, 0.3929]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.2923, 0.3586, 0.3491]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.2758, 0.3860, 0.3382]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3280, 0.3440, 0.3280]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2752, 0.3933, 0.3315]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 2\n",
      "tensor([[0.3086, 0.3423, 0.3491]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2814, 0.3719, 0.3466]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3161, 0.3475, 0.3364]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.2920, 0.3705, 0.3375]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3107, 0.3525, 0.3368]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3017, 0.3293, 0.3690]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3211, 0.3420, 0.3369]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.2865, 0.3771, 0.3364]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3135, 0.3287, 0.3578]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.2966, 0.4068, 0.2966]], device='cuda:0', dtype=torch.float64,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2806, 0.3760, 0.3434]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.2797, 0.4335, 0.2867]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3227, 0.3227, 0.3546]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2990, 0.3273, 0.3738]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3118, 0.3764, 0.3118]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2812, 0.4207, 0.2981]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.2629, 0.4006, 0.3365]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3273, 0.3273, 0.3455]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 2\n",
      "tensor([[0.2757, 0.3938, 0.3305]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3304, 0.3393, 0.3304]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.2923, 0.4155, 0.2923]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3251, 0.3251, 0.3499]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.2742, 0.4517, 0.2742]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3195, 0.3195, 0.3610]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.3215, 0.3344, 0.3441]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3086, 0.3377, 0.3537]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.2824, 0.4351, 0.2824]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3021, 0.3077, 0.3902]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3299, 0.3299, 0.3402]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 0\n",
      "tensor([[0.2852, 0.3988, 0.3160]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3165, 0.3165, 0.3671]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.2595, 0.4811, 0.2595]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.2871, 0.3847, 0.3282]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3118, 0.3559, 0.3323]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.2848, 0.3634, 0.3518]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.2765, 0.4037, 0.3198]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3089, 0.3439, 0.3472]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2749, 0.4169, 0.3082]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 2\n",
      "tensor([[0.2776, 0.4032, 0.3193]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3168, 0.3664, 0.3168]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.2960, 0.3555, 0.3485]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3139, 0.3721, 0.3139]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3307, 0.3385, 0.3307]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.2623, 0.4111, 0.3266]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2731, 0.4187, 0.3083]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 2\n",
      "tensor([[0.3166, 0.3287, 0.3548]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.3254, 0.3492, 0.3254]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3153, 0.3153, 0.3694]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 0\n",
      "tensor([[0.3225, 0.3225, 0.3549]], device='cuda:0', dtype=torch.float64,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.3332, 0.3336, 0.3332]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.2798, 0.3656, 0.3546]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.2815, 0.3965, 0.3220]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 2\n",
      "tensor([[0.3315, 0.3371, 0.3315]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.3179, 0.3331, 0.3490]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3214, 0.3214, 0.3573]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2764, 0.4473, 0.2764]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3308, 0.3383, 0.3308]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3048, 0.3459, 0.3493]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3301, 0.3397, 0.3301]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3040, 0.3290, 0.3671]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3217, 0.3566, 0.3217]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3079, 0.3079, 0.3841]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2364, 0.5272, 0.2364]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.2972, 0.4055, 0.2972]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.3155, 0.3458, 0.3387]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2856, 0.3691, 0.3454]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3043, 0.3652, 0.3305]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.2470, 0.4991, 0.2539]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.2781, 0.3792, 0.3426]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3241, 0.3241, 0.3518]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2949, 0.3618, 0.3432]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.2861, 0.4278, 0.2861]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.2925, 0.3817, 0.3257]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2661, 0.4321, 0.3018]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.2838, 0.3209, 0.3953]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.2850, 0.3843, 0.3306]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3138, 0.3591, 0.3272]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3074, 0.3450, 0.3476]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.2812, 0.4268, 0.2920]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 1\n",
      "tensor([[0.2822, 0.3282, 0.3897]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 0\n",
      "tensor([[0.3017, 0.3412, 0.3571]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.3112, 0.3320, 0.3568]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.2883, 0.3497, 0.3620]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 1\n",
      "tensor([[0.3072, 0.3072, 0.3856]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3054, 0.3054, 0.3892]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.2889, 0.4145, 0.2966]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2675, 0.4649, 0.2675]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3199, 0.3562, 0.3238]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3057, 0.3416, 0.3527]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3276, 0.3276, 0.3447]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3293, 0.3414, 0.3293]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 2\n",
      "tensor([[0.2785, 0.3922, 0.3293]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3230, 0.3230, 0.3540]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.2705, 0.4345, 0.2950]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.2914, 0.3505, 0.3581]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 2\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2849, 0.3608, 0.3543]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n",
      "tensor([[0.3065, 0.3244, 0.3691]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 2 labels max arguement is 1\n",
      "tensor([[0.3169, 0.3661, 0.3169]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 0\n",
      "tensor([[0.3333, 0.3333, 0.3333]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 0 labels max arguement is 0\n",
      "tensor([[0.2816, 0.3678, 0.3506]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)output's max arguement is 1 labels max arguement is 2\n"
     ]
    }
   ],
   "source": [
    "accu=[]\n",
    "for k in range(1): \n",
    "    i=random.randint(1,1000)\n",
    "    path1=r'C:\\Users\\occisor\\Downloads\\Engagement_recognition\\wacv2016-master\\1'\n",
    "    path2=r'C:\\Users\\occisor\\Downloads\\Engagement_recognition\\wacv2016-master\\2'\n",
    "    path3=r'C:\\Users\\occisor\\Downloads\\Engagement_recognition\\wacv2016-master\\3'\n",
    "    dest_path1=r'C:\\Users\\occisor\\Downloads\\Engagement_recognition\\wacv2016-master\\dataset\\1'\n",
    "    dest_path2=r'C:\\Users\\occisor\\Downloads\\Engagement_recognition\\wacv2016-master\\dataset\\2'\n",
    "    dest_path3=r'C:\\Users\\occisor\\Downloads\\Engagement_recognition\\wacv2016-master\\dataset\\3'\n",
    "\n",
    "    copy_files(path1,dest_path1)\n",
    "    copy_files(path2,dest_path2)\n",
    "    copy_files(path3,dest_path3)\n",
    "    \n",
    "    path=r'C:\\Users\\occisor\\Downloads\\Engagement_recognition\\wacv2016-master\\dataset'\n",
    "    \n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    }\n",
    "    image_datasets =datasets.ImageFolder(path,data_transforms['train'])\n",
    "\n",
    "    batch_size = 1\n",
    "    validation_split = .2\n",
    "    shuffle_dataset = True\n",
    "    random_seed= i\n",
    "\n",
    "    # Creating data indices for training and validation splits:\n",
    "    dataset_size = len(image_datasets)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(i)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(image_datasets, batch_size=batch_size, \n",
    "                                               sampler=train_sampler)\n",
    "    validation_loader = torch.utils.data.DataLoader(image_datasets, batch_size=batch_size,\n",
    "                                                    sampler=valid_sampler)\n",
    "\n",
    "    class LeNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(LeNet, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 6, kernel_size=3,padding=2)\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "            self.conv2 = nn.Conv2d(6, 9, kernel_size=3,padding=2)  \n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "            self.conv3 = nn.Conv2d(9,12, kernel_size=3,padding=2)\n",
    "            self.pool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "            self.fc1 = nn.Linear(2352,3)\n",
    "            \n",
    "\n",
    "        def forward(self, x):\n",
    "            x=F.relu(self.conv1(x))\n",
    "            x=self.pool1(x)\n",
    "            x=F.relu(self.conv2(x))\n",
    "            x=self.pool2(x)\n",
    "            x=F.relu(self.conv3(x))\n",
    "            x=self.pool3(x)\n",
    "            x=x.view((-1,2352))\n",
    "            x=F.relu(self.fc1(x))\n",
    "            x=F.softmax(x)\n",
    "            return x\n",
    "\n",
    "    net=LeNet()\n",
    "    net=net.double()\n",
    "    net=net.cuda()\n",
    "\n",
    "\n",
    "    InputData=Variable(torch.Tensor(1,3,100,100))\n",
    "    InputData=InputData.double()\n",
    "    InputData=InputData.cuda()\n",
    "    output=net(InputData)\n",
    "\n",
    "    criterion=nn.MSELoss() \n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-4) # Adam\n",
    "\n",
    "    \n",
    "    for epoch in tqdm(range(5)):\n",
    "        for data in train_loader:\n",
    "            inputs,labels=data\n",
    "            inputs=inputs.double()\n",
    "            inputs=Variable(inputs.cuda())\n",
    "            labels=torch.LongTensor([labels])\n",
    "            labels=one_hot_embedding(labels,3)\n",
    "            labels=labels.reshape(1,3)\n",
    "            labels=labels.double()\n",
    "            labels=Variable(labels.cuda())\n",
    "            net.zero_grad()\n",
    "            output=net(inputs)\n",
    "            loss=criterion(labels,output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    count=0\n",
    "    y_label = []\n",
    "    y_pred = []\n",
    "    net=net.eval()\n",
    "    for data in validation_loader:\n",
    "        inputs,labels=data\n",
    "        inputs=inputs.double()\n",
    "        inputs=Variable(inputs.cuda())\n",
    "        labels=torch.LongTensor([labels])\n",
    "        labels=one_hot_embedding(labels,3)\n",
    "        labels=labels.reshape(1,3)\n",
    "        labels=labels.double()\n",
    "        labels=Variable(labels.cuda())\n",
    "        output=net(inputs)\n",
    "        y_pred.append(output.argmax())\n",
    "        y_label.append(labels.argmax())\n",
    "        print(output,end=\"\")\n",
    "        print(\"output's max arguement is {}\".format(output.argmax()),end=\" \")\n",
    "        print(\"labels max arguement is {}\".format(labels.argmax()))\n",
    "        if output.argmax()==labels.argmax():\n",
    "            count+=1\n",
    "        \n",
    "    ac=count/len(valid_sampler)\n",
    "    accu.append(ac)\n",
    "    shutil.rmtree(dest_path1) \n",
    "    shutil.rmtree(dest_path2)\n",
    "    os.mkdir(dest_path1)\n",
    "    os.mkdir(dest_path2)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.asarray(y_pred)\n",
    "y_pred = y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = np.asarray(y_label)\n",
    "y_label = y_label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(y_label, y_pred,digit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.GI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.AGF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'Enagagement_analysis_37.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
